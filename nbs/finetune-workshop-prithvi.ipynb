{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc5e729-9116-4ec9-bf1e-8346cbccdf7b",
   "metadata": {},
   "source": [
    "## Run Prithvi\n",
    "\n",
    "An exercise to do a small but complete analysis from scratch. For this we will\n",
    "\n",
    "1. Set a location and date range of interest\n",
    "2. Download Lansat imagery for this specification\n",
    "3. Load the model checkpoint\n",
    "4. Prepare data into a format for the model\n",
    "5. Run the model on the imagery\n",
    "6. Analyise the model output (embeddings) using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e991e-3e73-48f6-b006-b6879458e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17b8a8-a9c6-4053-833e-de97287fae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "import stackstac\n",
    "import torch\n",
    "import yaml\n",
    "from einops import rearrange, reduce\n",
    "from huggingface_hub import hf_hub_download\n",
    "from matplotlib import pyplot as plt\n",
    "from prithvi import MaskedAutoencoderViT\n",
    "from rasterio.enums import Resampling\n",
    "from shapely import Point\n",
    "from sklearn import decomposition, svm\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac6394-9762-422b-9f5d-82d226018c0c",
   "metadata": {},
   "source": [
    "### Specify location and date of interest\n",
    "In this example we will use a location in Portugal where a forest fire happened. We will run the model over the time period of the fire and analyse the model embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7787d-1506-4de7-89dc-c1054910acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point over Monchique Portugal\n",
    "lat, lon = 37.30939, -8.57207\n",
    "\n",
    "# Dates of a large forest fire\n",
    "start = \"2018-07-01\"\n",
    "end = \"2018-09-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd226c9-003b-4867-a64a-8ae887e7e20a",
   "metadata": {},
   "source": [
    "### Get data from STAC catalog\n",
    "\n",
    "Based on the location and date we can obtain a stack of imagery using stackstac. Let's start with finding the STAC items we want to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80743c-7c77-459b-9984-f6c26cdff549",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAC_API = \"https://landsatlook.usgs.gov/stac-server\"\n",
    "COLLECTION = \"landsat-c2l2-sr\"\n",
    "\n",
    "# Search the catalogue\n",
    "catalog = pystac_client.Client.open(STAC_API)\n",
    "search = catalog.search(\n",
    "    collections=[COLLECTION],\n",
    "    datetime=f\"{start}/{end}\",\n",
    "    bbox=(lon - 1e-5, lat - 1e-5, lon + 1e-5, lat + 1e-5),\n",
    "    max_items=100,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 80}},\n",
    ")\n",
    "\n",
    "all_items = search.get_all_items()\n",
    "all_items\n",
    "item = all_items[0]\n",
    "item\n",
    "\n",
    "# Use S3 links for downloading imagery\n",
    "for item in all_items:\n",
    "    for key in item.assets.keys():\n",
    "        if \"alternate\" in item.assets[key].extra_fields:\n",
    "            url = urllib.parse.urlparse(\n",
    "                item.assets[key].extra_fields[\"alternate\"][\"s3\"][\"href\"]\n",
    "            )\n",
    "            item.assets[key].href = f\"https://{url.netloc}.s3.amazonaws.com{url.path}\"\n",
    "            item.assets[key].href = item.assets[key].extra_fields[\"alternate\"][\"s3\"][\n",
    "                \"href\"\n",
    "            ]\n",
    "\n",
    "# Reduce to LS8 and LS9\n",
    "items = []\n",
    "dates = []\n",
    "for item in all_items:\n",
    "    if item.datetime.date() not in dates:\n",
    "        if item.id.startswith(\"LC08\") or item.id.startswith(\"LC09\"):\n",
    "            items.append(item)\n",
    "            dates.append(item.datetime.date())\n",
    "\n",
    "\n",
    "print(f\"Found {len(items)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c68ae-7c8a-446a-8bc7-5afba70183c2",
   "metadata": {},
   "source": [
    "### Create a bounding box around the point of interest\n",
    "\n",
    "This is needed in the projection of the data so that we can generate image chips of the right size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3573b5-5a00-47d9-a648-5c4d7cd2c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinate system from first item\n",
    "epsg = items[0].properties[\"proj:epsg\"]\n",
    "\n",
    "# Convert point of interest into the image projection\n",
    "# (assumes all images are in the same projection)\n",
    "poidf = gpd.GeoDataFrame(\n",
    "    pd.DataFrame(),\n",
    "    crs=\"EPSG:4326\",\n",
    "    geometry=[Point(lon, lat)],\n",
    ").to_crs(epsg)\n",
    "\n",
    "coords = poidf.iloc[0].geometry.coords[0]\n",
    "\n",
    "# Create bounds in projection\n",
    "size = 224\n",
    "gsd = 30\n",
    "bounds = (\n",
    "    coords[0] - (size * gsd) // 2,\n",
    "    coords[1] - (size * gsd) // 2,\n",
    "    coords[0] + (size * gsd) // 2,\n",
    "    coords[1] + (size * gsd) // 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd3f67-5f2c-46dc-9ee1-2ef1f50fa032",
   "metadata": {},
   "source": [
    "### Retrieve the imagery data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d3824-e48c-4f9d-9c7b-181c0800f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_REQUEST_PAYER\"] = \"requester\"\n",
    "\n",
    "# Retrieve the pixel values, for the bounding box in\n",
    "# the target projection. In this example we use only\n",
    "# the RGB and NIR bands.\n",
    "stack = stackstac.stack(\n",
    "    items,\n",
    "    bounds=bounds,\n",
    "    snap_bounds=False,\n",
    "    epsg=epsg,\n",
    "    resolution=gsd,\n",
    "    dtype=\"float32\",\n",
    "    rescale=False,\n",
    "    fill_value=0,\n",
    "    assets=[\"blue\", \"green\", \"red\", \"nir08\", \"swir16\", \"swir22\"],\n",
    "    resampling=Resampling.nearest,\n",
    ")\n",
    "\n",
    "print(f\"Working with stack of size {stack.shape}\")\n",
    "\n",
    "stack = stack.compute()\n",
    "\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77354bee-bea3-43e0-8936-5808b352e25f",
   "metadata": {},
   "source": [
    "### Let's have a look at the imagery we just downloaded\n",
    "\n",
    "The imagery will contain 7 dates before the fire, of which two are pretty cloudy images. There are also 5 images after the forest fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468ab57-b44e-4099-9f95-794c803ccc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.sel(band=[\"red\", \"green\", \"blue\"]).plot.imshow(\n",
    "    row=\"time\", rgb=\"band\", vmin=0, vmax=20000, col_wrap=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa15e9-0285-4cac-816d-a58d2ceda389",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "We now have the data to analyse, let's load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb9b6f-00e4-45a1-b575-2484b5afd511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up config\n",
    "REPO_ID = \"ibm-nasa-geospatial/Prithvi-100M\"\n",
    "CONFIG = \"Prithvi_100M_config.yaml\"\n",
    "CHECKPOINT = \"Prithvi_100M.pt\"\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "NO_DATA = -9999\n",
    "NO_DATA_FLOAT = 0.0001\n",
    "\n",
    "# Download and load configuration and checkpoint\n",
    "config_path = hf_hub_download(repo_id=REPO_ID, filename=CONFIG)\n",
    "config = yaml.safe_load(open(config_path))\n",
    "checkpoint_path = hf_hub_download(repo_id=REPO_ID, filename=CHECKPOINT)\n",
    "\n",
    "# Initialize the model\n",
    "model_args = config[\"model_args\"]\n",
    "model = MaskedAutoencoderViT(**model_args)\n",
    "model = model.to(DEVICE)\n",
    "state_dict = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8e1f3-011d-4be5-8071-547f0ad91ad6",
   "metadata": {},
   "source": [
    "### Convert the band pixel data in to the format for the model\n",
    "\n",
    "We will take the information in the stack of imagery and convert it into the format that the model requires.\n",
    "\n",
    "For Prithvi, this means creating input triples. The model takes a short time series of 3 images as input.\n",
    "Since we do not have a lot of data in this example, we create the triples with overlapping dates. So \n",
    "the first triple is for the dates 1 to 3, the second one goes from date 2 to 4, and so on. This leads\n",
    "to six triples with the data we have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbe0c2-2cc3-428c-8d38-7e9516b6134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = config[\"train_params\"][\"data_mean\"]\n",
    "std = config[\"train_params\"][\"data_std\"]\n",
    "\n",
    "chips = []\n",
    "for i in range(6):\n",
    "    chips.append(stack.isel(time=slice(i, i + 3)).values)\n",
    "chips[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a18e52b-b21e-4c58-a1f2-26b66d73ecbe",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "\n",
    "Pass the datacube we prepared to the model to create embeddings. This will create one embedding vector for each of the images we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b22329-55e0-4d57-b384-9bd28ff34647",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array(mean)\n",
    "std = np.array(std)\n",
    "\n",
    "mean = mean[:, None, None]\n",
    "std = std[:, None, None]\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for ts in chips:\n",
    "    ts = rearrange(ts, \"t c h w -> 1 c t h w\")\n",
    "    ts = ts.astype(np.float32)\n",
    "    if ts.shape[2] == 3:\n",
    "        embedding = model.forward_encoder(torch.from_numpy(ts), mask_ratio=0.0)\n",
    "        cls_embedding = embedding[:, 0, :].detach().cpu().numpy().ravel()\n",
    "        embeddings.append(cls_embedding)\n",
    "\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3ebb8-18a2-4918-b863-01ea36095d9f",
   "metadata": {},
   "source": [
    "### Analyse the embeddings\n",
    "\n",
    "A simple analysis of the embeddings is to reduce each one of them into a single number using Principal Component Analysis. For this we will fit a PCA on the 12 embeddings we have, and do the dimensionality reduction for them. We will se a separation into three groups, the previous images, the cloudy images, and the images after the fire, they all fall into a different range of the PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8debf91-e38a-46d2-81c9-24b71a3adfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "# Plot all points in blue first\n",
    "plt.scatter(stack.time[:6], pca_result, color=\"blue\")\n",
    "\n",
    "# # Re-plot cloudy images in green\n",
    "plt.scatter(stack.time[0], pca_result[0], color=\"green\")\n",
    "\n",
    "# # Color all images after fire in red\n",
    "plt.scatter(stack.time[3:6], pca_result[3:6], color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae5fb9",
   "metadata": {},
   "source": [
    "### One embeding per time stamp\n",
    "\n",
    "Prithvi will also output embeddings for each time step that was passed to it.\n",
    "\n",
    "In this example we extract the three individual time step embeddings for each\n",
    "input triple and visualize those using PCA as in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fcd76-2fc3-4828-85f6-87210ada5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array(mean)\n",
    "std = np.array(std)\n",
    "\n",
    "mean = mean[:, None, None]\n",
    "std = std[:, None, None]\n",
    "\n",
    "long_embeddings = []\n",
    "\n",
    "for ts in chips:\n",
    "    ts = rearrange(ts, \"t c h w -> 1 c t h w\")\n",
    "    ts = ts.astype(np.float32)\n",
    "    if ts.shape[2] == 3:\n",
    "        embedding = model.forward_encoder(torch.from_numpy(ts), mask_ratio=0.0)\n",
    "        cls_embedding = embedding[:, 0, :].detach().cpu().numpy().ravel()\n",
    "        embedding = rearrange(embedding[:, 1:, :], \"1 (t n) d -> 1 t n d\", t=3)[0]\n",
    "        embedding = reduce(embedding, \"t n d -> t d\", \"mean\").detach().numpy()\n",
    "        t0, t1, t2 = embedding\n",
    "        long_embeddings.extend([t0] + [t1] + [t2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ebda4-23be-46f8-b232-b249883096dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "pca_result = pca.fit_transform(long_embeddings)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "# Plot all points in blue first\n",
    "plt.scatter(np.arange(18), pca_result, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b70a6-2156-41f8-967e-a490cc8e2778",
   "metadata": {},
   "source": [
    "### And finally, some finetuning\n",
    "\n",
    "We are going to train a classifier head on the embeddings and use it to detect fires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da07de0-b8f2-46c9-bd2a-58b15ca2224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the images we downloaded\n",
    "# 0 = Cloud\n",
    "# 1 = Forest\n",
    "# 2 = Fire\n",
    "labels = np.array([0, 1, 1, 2, 2, 2])\n",
    "\n",
    "# Split into fit and test manually, ensuring we have all 3 classes in both sets\n",
    "fit = [0, 1, 3]\n",
    "test = [2, 4, 5]\n",
    "\n",
    "# Train a support vector machine model\n",
    "clf = svm.SVC()\n",
    "clf.fit(embeddings[fit], labels[fit])\n",
    "\n",
    "# Predict classes on test set\n",
    "prediction = clf.predict(embeddings[test])\n",
    "\n",
    "# Perfect match for SVM\n",
    "match = np.sum(labels[test] == prediction)\n",
    "print(f\"Matched {match} out of {len(test)} correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f25e33-8b8d-46e0-bdeb-5d5ed3963dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive bayes does not learn about the clouds\n",
    "clf = MultinomialNB()\n",
    "clf.fit(embeddings[fit] + 100, labels[fit])\n",
    "\n",
    "# Predict classes on test set\n",
    "prediction = clf.predict(embeddings[test] + 100)\n",
    "\n",
    "match = np.sum(labels[test] == prediction)\n",
    "print(f\"Matched {match} out of {len(test)} correctly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
