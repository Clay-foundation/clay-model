{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e66cec10-75b7-4a14-b9b4-91f4d707bb6d",
   "metadata": {},
   "source": [
    "# CLAY Embeddings Feature detection\n",
    "\n",
    "The goal of this notebook is to show how to use the CLAY embeddings to detect features. We will use embeddgins for a Bali, and labels where aquaculture is present. We'll tip the embeddgins with a few examples of locations of aquaculture, and we'll see how the embeddings can be used to detect other locations of aquaculture. We can iterate refining positive and negative examples to improve the detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65130e67-0868-4e6e-b181-4c456223f998",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0176a6-97a1-4af6-af75-b9e52e52fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea314d0-176a-4ee3-b738-6152d27275d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "from shapely.wkb import loads\n",
    "from shapely import wkt\n",
    "\n",
    "\n",
    "from src.datamodule import ClayDataModule, ClayDataset\n",
    "from src.model_clay import CLAYModule\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b28e1-c8c6-470d-8b38-5600e4897074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read embeddings geoparquet file\n",
    "data_dir = Path(\"../datadisk\")\n",
    "embeddings_file = data_dir / \"aquaculture_processed.gpq\"\n",
    "embeddings = pq.read_table(embeddings_file).to_pandas()\n",
    "\n",
    "embeddings['geometry'] = embeddings['geometry'].apply(lambda x: loads(x).wkt)\n",
    "embeddings['geometry'] = embeddings['geometry'].apply(wkt.loads)\n",
    "\n",
    "# Calculate the centroid and create new columns for x and y coordinates\n",
    "embeddings['x'] = embeddings['geometry'].apply(lambda geom: geom.centroid.x)\n",
    "embeddings['y'] = embeddings['geometry'].apply(lambda geom: geom.centroid.y)\n",
    "\n",
    "\n",
    "print(len(embeddings))\n",
    "#print number of occurrences of each unique value in the 'aquaculture' column\n",
    "print(embeddings['aquaculture'].value_counts())\n",
    "print(len(embeddings.iloc[0]['embeddings']))\n",
    "embeddings.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "gdf = embeddings.copy()\n",
    "\n",
    "n_initial_pos = 2  # Initial number of positive training samples\n",
    "n_initial_neg = 0  # Initial number of negative training samples\n",
    "n_add_pos_per_iter = 3  # Number of positive samples to add to the training set per iteration\n",
    "n_add_neg_per_iter = 3  # Number of negative samples to add to the training set per iteration\n",
    "max_iterations = 100\n",
    "max_drop_percentage = 0.1  # % of features to drop per iteration\n",
    "early_stopping_rounds = 10  # Number of iterations without improvement for early stopping\n",
    "\n",
    "# Split the data into positive and negative samples\n",
    "pos_samples = gdf[gdf['aquaculture'] == 1]\n",
    "neg_samples = gdf[gdf['aquaculture'] == 0]\n",
    "\n",
    "# Split the positive samples into training and test sets\n",
    "pos_train_indices = pos_samples.sample(n=n_initial_pos, random_state=42).index\n",
    "pos_test_indices = pos_samples.drop(pos_train_indices).index\n",
    "\n",
    "# Split the negative samples into training and test sets\n",
    "neg_train_indices = neg_samples.sample(n=n_initial_neg, random_state=42).index\n",
    "neg_test_indices = neg_samples.drop(neg_train_indices).index\n",
    "\n",
    "# Combine the positive and negative training and test indices\n",
    "train_indices = np.concatenate((pos_train_indices, neg_train_indices))\n",
    "test_indices = np.concatenate((pos_test_indices, neg_test_indices))\n",
    "\n",
    "# Initialize lists to store accuracy, number of dimensions, and iteration number\n",
    "accuracies = []\n",
    "dimensions = []\n",
    "iterations = []\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Iterate until maximum iterations are reached or early stopping criteria is met\n",
    "iteration = 0\n",
    "best_accuracy = 0\n",
    "no_improvement_count = 0\n",
    "while iteration < max_iterations:\n",
    "    # Select training samples from the training indices\n",
    "    train_samples = gdf.loc[train_indices]\n",
    "    X_train = np.array(train_samples['embeddings'].tolist())\n",
    "    y_train = train_samples['aquaculture'].values\n",
    "\n",
    "    # Train the classifier on the selected training samples\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Extract feature importances and identify least important dimensions to drop\n",
    "    importances = rf.feature_importances_\n",
    "    drop_threshold = np.min(importances) * (1 + max_drop_percentage)\n",
    "    drop_mask = importances <= drop_threshold\n",
    "\n",
    "    # Calculate the number of features to drop, adhering to max_drop_percentage\n",
    "    num_features_to_drop = int(max_drop_percentage * len(importances))\n",
    "\n",
    "    # If more than max_drop_percentage of features have importance below the drop_threshold\n",
    "    if np.sum(drop_mask) > num_features_to_drop:\n",
    "        print(f\"More than {max_drop_percentage * 100}% of features have importance below {drop_threshold}.\")\n",
    "        # Select a random subset of these features to drop, adhering to the max_drop_percentage\n",
    "        least_important_indices = np.where(drop_mask)[0]\n",
    "        least_important_dims = random.sample(list(least_important_indices), num_features_to_drop)\n",
    "    else:\n",
    "        # If not exceeding max_drop_percentage, proceed with dropping all identified features\n",
    "        least_important_dims = np.where(drop_mask)[0]\n",
    "\n",
    "    print(f\"Dropping {len(least_important_dims)} dimensions randomly selected among those with importances below {drop_threshold}\")\n",
    "\n",
    "    # Drop the least important dimensions from the entire dataset\n",
    "    gdf['embeddings'] = gdf['embeddings'].apply(lambda x: np.delete(x, least_important_dims))\n",
    "\n",
    "    # Select training samples from the training indices\n",
    "    train_samples = gdf.loc[train_indices]\n",
    "    X_train = np.array(train_samples['embeddings'].tolist())\n",
    "    y_train = train_samples['aquaculture'].values\n",
    "\n",
    "    # Train the classifier on the selected training samples\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Select testing samples from the test indices\n",
    "    test_samples = gdf.loc[test_indices]\n",
    "    X_test = np.array(test_samples['embeddings'].tolist())\n",
    "    y_test = test_samples['aquaculture'].values\n",
    "\n",
    "    # Evaluate the accuracy of the classifier on the testing samples\n",
    "    accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "    # Store the accuracy, number of dimensions, and iteration number\n",
    "    accuracies.append(accuracy)\n",
    "    dimensions.append(len(gdf.iloc[0]['embeddings']))\n",
    "    iterations.append(iteration)\n",
    "\n",
    "    # Print the results for the current iteration\n",
    "    print(f\"Iteration: {iteration}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Number of dimensions: {len(gdf.iloc[0]['embeddings'])}\")\n",
    "    print(f\"Number of positive training samples: {len(pos_train_indices)}\")\n",
    "    print(f\"Number of negative training samples: {len(neg_train_indices)}\")\n",
    "    print(f\"Number of positive test samples: {len(pos_test_indices)}\")\n",
    "    print(f\"Number of negative test samples: {len(neg_test_indices)}\")\n",
    "    print()\n",
    "\n",
    "    # Check for early stopping\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "        if no_improvement_count >= early_stopping_rounds:\n",
    "            print(f\"No improvement in accuracy for {early_stopping_rounds} iterations. Early stopping.\")\n",
    "            break\n",
    "\n",
    "    # Move samples from test set to training set\n",
    "    if len(pos_test_indices) >= n_add_pos_per_iter and len(neg_test_indices) >= n_add_neg_per_iter:\n",
    "        pos_train_indices = np.concatenate((pos_train_indices, pos_test_indices[:n_add_pos_per_iter]))\n",
    "        neg_train_indices = np.concatenate((neg_train_indices, neg_test_indices[:n_add_neg_per_iter]))\n",
    "        pos_test_indices = pos_test_indices[n_add_pos_per_iter:]\n",
    "        neg_test_indices = neg_test_indices[n_add_neg_per_iter:]\n",
    "    else:\n",
    "        pos_train_indices = np.concatenate((pos_train_indices, pos_test_indices))\n",
    "        neg_train_indices = np.concatenate((neg_train_indices, neg_test_indices))\n",
    "        pos_test_indices = []\n",
    "        neg_test_indices = []\n",
    "        print(\"All test samples moved to the training set. Stopping iterations.\")\n",
    "        break\n",
    "\n",
    "    # Update the training and test indices\n",
    "    train_indices = np.concatenate((pos_train_indices, neg_train_indices))\n",
    "    test_indices = np.concatenate((pos_test_indices, neg_test_indices))\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "# Create custom x-tick labels with iterations and dimensions\n",
    "xtick_labels = [f\"{iteration} [{dimension}]\" for iteration, dimension in zip(iterations, dimensions)]\n",
    "\n",
    "# Plot the accuracy versus the iteration number\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, accuracies, marker='o')\n",
    "plt.xlabel('Iteration [Dimensions]')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Iteration')\n",
    "plt.grid(True)\n",
    "plt.xticks(iterations, xtick_labels, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the final number of dimensions and the corresponding accuracy\n",
    "print(f\"Final number of dimensions: {len(gdf.iloc[0]['embeddings'])}\")\n",
    "print(f\"Final accuracy: {accuracies[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9721ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import folium\n",
    "\n",
    "def plot_satellite_image(bbox):\n",
    "    \"\"\"\n",
    "    Plot the satellite image of a given bounding box (bbox).\n",
    "    \n",
    "    Parameters:\n",
    "    - bbox (tuple): A tuple of (min_lat, min_lon, max_lat, max_lon)\n",
    "    \n",
    "    Returns:\n",
    "    - A folium map object with the satellite image of the given bbox.\n",
    "    \"\"\"\n",
    "    # Calculate the center of the bounding box\n",
    "    center_lat = (bbox[0] + bbox[2]) / 2\n",
    "    center_lon = (bbox[1] + bbox[3]) / 2\n",
    "    \n",
    "    # Create a folium map centered at the calculated center\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=13, tiles='Esri.WorldImagery', attr='Custom Attribution')\n",
    "    \n",
    "    # Add the bounding box as a rectangle on the map\n",
    "    folium.Rectangle(\n",
    "        bounds=[(bbox[0], bbox[1]), (bbox[2], bbox[3])],\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "def calculate_precision(all_embeddings,\n",
    "                        positive_labels, negative_labels,\n",
    "                        drop_percentage):\n",
    "\n",
    "    all_embeddings.reset_index(drop=True, inplace=True)\n",
    "    pos_samples = all_embeddings[all_embeddings['aquaculture'] == 1]\n",
    "    neg_samples = all_embeddings[all_embeddings['aquaculture'] == 0]\n",
    "\n",
    "    # extract positive and negative samples from the all_embeddings, put rest in test set\n",
    "    pos_train_indices = pos_samples.sample(n=positive_labels).index\n",
    "    neg_train_indices = neg_samples.sample(n=negative_labels).index\n",
    "    pos_test_indices = pos_samples.drop(pos_train_indices).index\n",
    "    neg_test_indices = neg_samples.drop(neg_train_indices).index\n",
    "\n",
    "    train_indices = np.concatenate((pos_train_indices, neg_train_indices))\n",
    "    test_indices = np.concatenate((pos_test_indices, neg_test_indices))\n",
    "\n",
    "    #shuffle the indices\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    train_samples = all_embeddings.loc[train_indices]\n",
    "    X_train = np.array(train_samples['embeddings'].tolist())\n",
    "    y_train = train_samples['aquaculture'].values\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    if drop_percentage > 0:\n",
    "        importances = rf.feature_importances_\n",
    "        # Sort importances and find the threshold importance that marks the top (1 - drop_percentage) of features\n",
    "        sorted_importances = np.sort(importances)\n",
    "        threshold_index = int(len(importances) * (1 - drop_percentage))\n",
    "        # The threshold importance is the value below which features will be considered for dropping\n",
    "        drop_threshold = sorted_importances[threshold_index]\n",
    "        \n",
    "        # Identify all features that have importance less than or equal to the threshold\n",
    "        drop_mask = importances <= drop_threshold\n",
    "        least_important_indices = np.where(drop_mask)[0]\n",
    "        \n",
    "        # If the number of features to drop is more specific, adjust the selection\n",
    "        num_features_to_drop = min(int(drop_percentage * len(importances)), len(least_important_indices))\n",
    "        \n",
    "        # Randomly select from the least important features if there are more than needed\n",
    "        if len(least_important_indices) > num_features_to_drop:\n",
    "            least_important_dims = random.sample(list(least_important_indices), num_features_to_drop)\n",
    "        else:\n",
    "            least_important_dims = least_important_indices\n",
    "\n",
    "        #print(f\"Dropping {len(least_important_dims)} dimensions from a total of {len(importances)}\")\n",
    "        all_embeddings['embeddings'] = all_embeddings['embeddings'].apply(lambda x: np.delete(x, least_important_dims))\n",
    "\n",
    "\n",
    "\n",
    "    #Get 25 closest cosine similarity to the centroid of the positive class, use pos_train_indices\n",
    "    centroid = np.mean(np.array(all_embeddings.iloc[pos_train_indices]['embeddings'].tolist()), axis=0)\n",
    "\n",
    "    # Calculate cosine similarity between the centroid and all embeddings\n",
    "    cosine_similarities = cosine_similarity([centroid], np.array(all_embeddings['embeddings'].tolist())).flatten() \n",
    "    #check how many of the pos_train_indices are in the top 25 closest  \n",
    "    closest_indices = np.argsort(cosine_similarities)\n",
    "    confirmed_indices = [i for i in closest_indices if i in pos_train_indices]\n",
    "    print(f\"Confirmed indices in top 25 closest: {len(confirmed_indices)}\")\n",
    "    # Exclude the positive training set indices from the closest_indices\n",
    "    closest_indices = [i for i in closest_indices if i not in pos_train_indices][:25]\n",
    "    closest_indices = closest_indices[:25]\n",
    "    closest_labels = all_embeddings.iloc[closest_indices]['aquaculture'].values\n",
    "    \n",
    "    # Calculate metrics for the 25 closest embeddings\n",
    "    closest_precision = precision_score([1]*25, closest_labels)\n",
    "    closest_recall = recall_score([1]*25, closest_labels)\n",
    "    closest_accuracy = accuracy_score([1]*25, closest_labels)\n",
    "    closest_f1 = 2 * (closest_precision * closest_recall) / (closest_precision + closest_recall) if (closest_precision + closest_recall) > 0 else 0\n",
    "    \n",
    "    print(f\"Closest 25 Precision: {closest_precision}\")\n",
    "    print(f\"Closest 25 Recall: {closest_recall}\")\n",
    "    print(f\"Closest 25 Accuracy: {closest_accuracy}\")\n",
    "    print(f\"Closest 25 F1 Score: {closest_f1}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Store the precision, number of dimensions, and iteration numbers\n",
    "    results = {\n",
    "        'precision': format(closest_precision, '.2f'),\n",
    "        'recall': format(closest_recall, '.2f'),\n",
    "        'accuracy': format(closest_accuracy, '.2f'),\n",
    "        'f1': format(closest_f1, '.2f'),\n",
    "        'num_positives': len(pos_train_indices),\n",
    "        'num_negatives': len(neg_train_indices),\n",
    "        'num_test': len(test_indices),\n",
    "        'num_dimensions': len(all_embeddings.iloc[0]['embeddings']),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "positive_labels = 36\n",
    "negative_labels = 36\n",
    "drop_percentage = .50\n",
    "gdf = embeddings.copy().reset_index(drop=True)\n",
    "\n",
    "calculate_precision(gdf, positive_labels, negative_labels,drop_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_initial_pos = 5  # Initial number of positive training samples\n",
    "n_initial_neg = 2  # Initial number of negative training samples\n",
    "n_add = 10  # Number of positive and 2* negative samples to add  per iteration\n",
    "drop_percentage = 0.1  # max_drop_percentage\n",
    "max_iterations = 500\n",
    "early_stopping_rounds = 50  # Number of iterations without improvement for early stopping\n",
    "\n",
    "\n",
    "# Iterate until maximum iterations are reached or early stopping criteria is met\n",
    "iteration = 0\n",
    "best_precision = 0\n",
    "precisions = []\n",
    "dimensions = []\n",
    "num_labels = []\n",
    "\n",
    "\n",
    "#traverse grid of adding positive/negative samples and dropping dimensions\n",
    "all_embeddings = embeddings.copy()\n",
    "\n",
    "for add_labels in np.arange(1, 200 , n_add):\n",
    "    positive_labels = n_initial_pos + add_labels\n",
    "    negative_labels = n_initial_neg + add_labels*2\n",
    "\n",
    "    for drop_percentage in np.arange(0, .7, drop_percentage):\n",
    "        iteration+=1\n",
    "        \n",
    "        results = calculate_precision(all_embeddings,\n",
    "                                    positive_labels, negative_labels,\n",
    "                                    drop_percentage)\n",
    "\n",
    "        print(f\"Iteration: {iteration} \", end=\"\")\n",
    "        for key, value in results.items():\n",
    "            print(f\"{key}: {value} \", end=\"\")\n",
    "        print(\"\")\n",
    "\n",
    "        precisions.append(results['precision'])\n",
    "        dimensions.append(results['num_dimensions'])\n",
    "        num_labels.append(positive_labels + negative_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "scatter = ax.scatter(dimensions, num_labels, c=precisions, cmap='viridis', vmin=0, vmax=1)\n",
    "cbar = fig.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Precision')\n",
    "ax.set_xlabel('num_dimensions')\n",
    "ax.set_ylabel('num_labels')\n",
    "ax.set_title('Precision by Iterations')\n",
    "ax.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def plot_satellite_image(bbox):\n",
    "    \"\"\"\n",
    "    Plot the satellite image of a given bounding box (bbox).\n",
    "    \n",
    "    Parameters:\n",
    "    - bbox (tuple): A tuple of (min_lat, min_lon, max_lat, max_lon)\n",
    "    \n",
    "    Returns:\n",
    "    - A folium map object with the satellite image of the given bbox.\n",
    "    \"\"\"\n",
    "    # Calculate the center of the bounding box\n",
    "    center_lat = (bbox[0] + bbox[2]) / 2\n",
    "    center_lon = (bbox[1] + bbox[3]) / 2\n",
    "    \n",
    "    # Create a folium map centered at the calculated center\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=13, tiles='Esri.WorldImagery', attr='Custom Attribution')\n",
    "    \n",
    "    # Add the bounding box as a rectangle on the map\n",
    "    folium.Rectangle(\n",
    "        bounds=[(bbox[0], bbox[1]), (bbox[2], bbox[3])],\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def calculate_precision(all_embeddings, positive_labels, negative_labels, drop_percentage):\n",
    "    all_embeddings.reset_index(drop=True, inplace=True)\n",
    "    pos_samples = all_embeddings[all_embeddings['aquaculture'] == 1]\n",
    "    neg_samples = all_embeddings[all_embeddings['aquaculture'] == 0]\n",
    "\n",
    "    # Extract positive and negative samples from all_embeddings, put rest in test set\n",
    "    pos_train_indices = pos_samples.sample(n=positive_labels).index\n",
    "    neg_train_indices = neg_samples.sample(n=negative_labels).index\n",
    "    pos_test_indices = pos_samples.drop(pos_train_indices).index\n",
    "    neg_test_indices = neg_samples.drop(neg_train_indices).index\n",
    "\n",
    "    train_indices = np.concatenate((pos_train_indices, neg_train_indices))\n",
    "    test_indices = np.concatenate((pos_test_indices, neg_test_indices))\n",
    "\n",
    "    # Shuffle the indices\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    train_samples = all_embeddings.loc[train_indices]\n",
    "    X_train = np.array(train_samples['embeddings'].tolist())\n",
    "    y_train = train_samples['aquaculture'].values\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    if drop_percentage > 0:\n",
    "        importances = rf.feature_importances_\n",
    "        # Sort importances and find the threshold importance that marks the top (1 - drop_percentage) of features\n",
    "        sorted_importances = np.sort(importances)\n",
    "        threshold_index = int(len(importances) * (1 - drop_percentage))\n",
    "        # The threshold importance is the value below which features will be considered for dropping\n",
    "        drop_threshold = sorted_importances[threshold_index]\n",
    "        \n",
    "        # Identify all features that have importance less than or equal to the threshold\n",
    "        drop_mask = importances <= drop_threshold\n",
    "        least_important_indices = np.where(drop_mask)[0]\n",
    "        \n",
    "        # If the number of features to drop is more specific, adjust the selection\n",
    "        num_features_to_drop = min(int(drop_percentage * len(importances)), len(least_important_indices))\n",
    "        \n",
    "        # Randomly select from the least important features if there are more than needed\n",
    "        if len(least_important_indices) > num_features_to_drop:\n",
    "            least_important_dims = random.sample(list(least_important_indices), num_features_to_drop)\n",
    "        else:\n",
    "            least_important_dims = least_important_indices\n",
    "\n",
    "        all_embeddings['embeddings'] = all_embeddings['embeddings'].apply(lambda x: np.delete(x, least_important_dims))\n",
    "\n",
    "    # Get 25 closest cosine similarity to the centroid of the positive class, use pos_train_indices\n",
    "    centroid = np.mean(np.array(all_embeddings.iloc[pos_train_indices]['embeddings'].tolist()), axis=0)\n",
    "\n",
    "    # Calculate cosine similarity between the centroid and all embeddings\n",
    "    cosine_similarities = cosine_similarity([centroid], np.array(all_embeddings['embeddings'].tolist())).flatten()\n",
    "    \n",
    "    # Exclude the positive training set indices from the closest_indices\n",
    "    closest_indices = np.argsort(cosine_similarities)\n",
    "    closest_indices = [i for i in closest_indices if i not in pos_train_indices][:25]\n",
    "    \n",
    "    # Get the true labels and predicted labels for the closest indices\n",
    "    closest_true_labels = all_embeddings.iloc[closest_indices]['aquaculture'].values\n",
    "    closest_pred_labels = np.ones(25)\n",
    "    \n",
    "    # Calculate metrics for the 25 closest embeddings\n",
    "    closest_precision = precision_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_recall = recall_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_accuracy = accuracy_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_f1 = f1_score(closest_true_labels, closest_pred_labels)\n",
    "    \n",
    "    # Create a folium map to visualize the locations\n",
    "    m = folium.Map(location=[all_embeddings['y'].mean(), \n",
    "                             all_embeddings['x'].mean()], \n",
    "                   zoom_start=9)\n",
    "    \n",
    "    # Add markers for positive training locations\n",
    "    for idx in pos_train_indices:\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        folium.CircleMarker(location=[row['y'], row['x']], \n",
    "                            radius=5, color='green', fill=True, fill_color='green', \n",
    "                            fill_opacity=0.7, popup=f\"Train Positive: {idx}\").add_to(m)\n",
    "    \n",
    "    # Add markers for negative training locations\n",
    "    for idx in neg_train_indices:\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        folium.CircleMarker(location=[row['y'], row['x']], \n",
    "                            radius=5, color='red', fill=True, fill_color='red', \n",
    "                            fill_opacity=0.7, popup=f\"Train Negative: {idx}\").add_to(m)\n",
    "    \n",
    "    # Add markers for closest 25 predictions\n",
    "    for idx, true_label in zip(closest_indices, closest_true_labels):\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        if true_label == 1:\n",
    "            color = 'blue'\n",
    "            popup = f\"Closest 25 True Positive: {idx}\"\n",
    "        else:\n",
    "            color = 'orange'\n",
    "            popup = f\"Closest 25 False Positive: {idx}\"\n",
    "        folium.CircleMarker(location=[row['y'], row['x']], \n",
    "                            radius=5, color=color, fill=True, fill_color=color, \n",
    "                            fill_opacity=0.7, popup=popup).add_to(m)\n",
    "    \n",
    "    # Display the map\n",
    "    display(m)\n",
    "    \n",
    "    # Store the precision, number of dimensions, and iteration numbers\n",
    "    results = {\n",
    "        'precision': format(closest_precision, '.2f'),\n",
    "        'recall': format(closest_recall, '.2f'),\n",
    "        'accuracy': format(closest_accuracy, '.2f'),\n",
    "        'f1': format(closest_f1, '.2f'),\n",
    "        'num_positives': len(pos_train_indices),\n",
    "        'num_negatives': len(neg_train_indices),\n",
    "        'num_test': len(test_indices),\n",
    "        'num_dimensions': len(all_embeddings.iloc[0]['embeddings']),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "positive_labels = 36\n",
    "negative_labels = 36\n",
    "drop_percentage = 0.50\n",
    "gdf = embeddings.copy().reset_index(drop=True)\n",
    "print(gdf.columns)\n",
    "\n",
    "results = calculate_precision(gdf, positive_labels, negative_labels, drop_percentage)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0dabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def plot_satellite_image(bbox):\n",
    "    \"\"\"\n",
    "    Plot the satellite image of a given bounding box (bbox).\n",
    "    \n",
    "    Parameters:\n",
    "    - bbox (tuple): A tuple of (min_lat, min_lon, max_lat, max_lon)\n",
    "    \n",
    "    Returns:\n",
    "    - A folium map object with the satellite image of the given bbox.\n",
    "    \"\"\"\n",
    "    # Calculate the center of the bounding box\n",
    "    center_lat = (bbox[0] + bbox[2]) / 2\n",
    "    center_lon = (bbox[1] + bbox[3]) / 2\n",
    "    \n",
    "    # Create a folium map centered at the calculated center\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=13, tiles='Esri.WorldImagery', attr='Custom Attribution')\n",
    "    \n",
    "    # Add the bounding box as a rectangle on the map\n",
    "    folium.Rectangle(\n",
    "        bounds=[(bbox[0], bbox[1]), (bbox[2], bbox[3])],\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def calculate_precision(all_embeddings, positive_labels, negative_labels, drop_percentage):\n",
    "    all_embeddings.reset_index(drop=True, inplace=True)\n",
    "    pos_samples = all_embeddings[all_embeddings['aquaculture'] == 1]\n",
    "    neg_samples = all_embeddings[all_embeddings['aquaculture'] == 0]\n",
    "\n",
    "    # Extract positive and negative samples from all_embeddings, put rest in test set\n",
    "    pos_train_indices = pos_samples.sample(n=positive_labels).index\n",
    "    neg_train_indices = neg_samples.sample(n=negative_labels).index\n",
    "    pos_test_indices = pos_samples.drop(pos_train_indices).index\n",
    "    neg_test_indices = neg_samples.drop(neg_train_indices).index\n",
    "\n",
    "    train_indices = np.concatenate((pos_train_indices, neg_train_indices))\n",
    "    test_indices = np.concatenate((pos_test_indices, neg_test_indices))\n",
    "\n",
    "    # Shuffle the indices\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    train_samples = all_embeddings.loc[train_indices]\n",
    "    X_train = np.array(train_samples['embeddings'].tolist())\n",
    "    y_train = train_samples['aquaculture'].values\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    if drop_percentage > 0:\n",
    "        importances = rf.feature_importances_\n",
    "        # Sort importances and find the threshold importance that marks the top (1 - drop_percentage) of features\n",
    "        sorted_importances = np.sort(importances)\n",
    "        threshold_index = int(len(importances) * (1 - drop_percentage))\n",
    "        # The threshold importance is the value below which features will be considered for dropping\n",
    "        drop_threshold = sorted_importances[threshold_index]\n",
    "        \n",
    "        # Identify all features that have importance less than or equal to the threshold\n",
    "        drop_mask = importances <= drop_threshold\n",
    "        least_important_indices = np.where(drop_mask)[0]\n",
    "        \n",
    "        # If the number of features to drop is more specific, adjust the selection\n",
    "        num_features_to_drop = min(int(drop_percentage * len(importances)), len(least_important_indices))\n",
    "        \n",
    "        # Randomly select from the least important features if there are more than needed\n",
    "        if len(least_important_indices) > num_features_to_drop:\n",
    "            least_important_dims = random.sample(list(least_important_indices), num_features_to_drop)\n",
    "        else:\n",
    "            least_important_dims = least_important_indices\n",
    "\n",
    "        all_embeddings['embeddings'] = all_embeddings['embeddings'].apply(lambda x: np.delete(x, least_important_dims))\n",
    "\n",
    "    # Get 25 closest cosine similarity to the centroid of the positive class, use pos_train_indices\n",
    "    centroid = np.mean(np.array(all_embeddings.iloc[pos_train_indices]['embeddings'].tolist()), axis=0)\n",
    "\n",
    "    # Calculate cosine similarity between the centroid and all embeddings\n",
    "    cosine_similarities = cosine_similarity([centroid], np.array(all_embeddings['embeddings'].tolist())).flatten()\n",
    "    \n",
    "    # Exclude the positive training set indices from the closest_indices\n",
    "    closest_indices = np.argsort(cosine_similarities)\n",
    "    closest_indices = [i for i in closest_indices if i not in pos_train_indices][:25]\n",
    "    \n",
    "    # Get the true labels and predicted labels for the closest indices\n",
    "    closest_true_labels = all_embeddings.iloc[closest_indices]['aquaculture'].values\n",
    "    closest_pred_labels = np.ones(25)\n",
    "    \n",
    "    # Calculate metrics for the 25 closest embeddings\n",
    "    closest_precision = precision_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_recall = recall_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_accuracy = accuracy_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_f1 = f1_score(closest_true_labels, closest_pred_labels)\n",
    "    \n",
    "    # Create a folium map to visualize the locations\n",
    "    m = folium.Map(location=[all_embeddings['y'].mean(), \n",
    "                             all_embeddings['x'].mean()], \n",
    "                   zoom_start=9)\n",
    "    \n",
    "    # Add markers for positive training locations\n",
    "    for idx in pos_train_indices:\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        geometry = row['geometry'] \n",
    "        folium.GeoJson(geometry).add_to(m)\n",
    "    \n",
    "    # Add markers for negative training locations\n",
    "    for idx in neg_train_indices:\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        geometry = row['geometry']\n",
    "        folium.GeoJson(geometry).add_to(m)\n",
    "    \n",
    "    # Add markers for closest 25 predictions\n",
    "    for idx, true_label in zip(closest_indices, closest_true_labels):\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        geometry = row['geometry']\n",
    "        if true_label == 1:\n",
    "            color = 'blue'\n",
    "            popup = f\"Closest 25 True Positive: {idx}\"\n",
    "        else:\n",
    "            color = 'orange'\n",
    "            popup = f\"Closest 25 False Positive: {idx}\"\n",
    "        folium.GeoJson(geometry, style_function=lambda x: {'fillColor': color}).add_to(m)\n",
    "    \n",
    "    # Display the map\n",
    "    display(m)\n",
    "    \n",
    "    # Store the precision, number of dimensions, and iteration numbers\n",
    "    results = {\n",
    "        'precision': format(closest_precision, '.2f'),\n",
    "        'recall': format(closest_recall, '.2f'),\n",
    "        'accuracy': format(closest_accuracy, '.2f'),\n",
    "        'f1': format(closest_f1, '.2f'),\n",
    "        'num_positives': len(pos_train_indices),\n",
    "        'num_negatives': len(neg_train_indices),\n",
    "        'num_test': len(test_indices),\n",
    "        'num_dimensions': len(all_embeddings.iloc[0]['embeddings']),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "positive_labels = 36\n",
    "negative_labels = 36\n",
    "drop_percentage = 0.50\n",
    "gdf = embeddings.copy().reset_index(drop=True)\n",
    "print(gdf.columns)\n",
    "\n",
    "results = calculate_precision(gdf, positive_labels, negative_labels, drop_percentage)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def plot_satellite_image(bbox):\n",
    "    \"\"\"\n",
    "    Plot the satellite image of a given bounding box (bbox).\n",
    "    \n",
    "    Parameters:\n",
    "    - bbox (tuple): A tuple of (min_lat, min_lon, max_lat, max_lon)\n",
    "    \n",
    "    Returns:\n",
    "    - A folium map object with the satellite image of the given bbox.\n",
    "    \"\"\"\n",
    "    # Calculate the center of the bounding box\n",
    "    center_lat = (bbox[0] + bbox[2]) / 2\n",
    "    center_lon = (bbox[1] + bbox[3]) / 2\n",
    "    \n",
    "    # Create a folium map centered at the calculated center\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=13, tiles='Esri.WorldImagery', attr='Custom Attribution')\n",
    "    \n",
    "    # Add the bounding box as a rectangle on the map\n",
    "    folium.Rectangle(\n",
    "        bounds=[(bbox[0], bbox[1]), (bbox[2], bbox[3])],\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def calculate_precision(all_embeddings, positive_labels, negative_labels, \n",
    "                        drop_percentage,closest_similar):\n",
    "    all_embeddings.reset_index(drop=True, inplace=True)\n",
    "    pos_samples = all_embeddings[all_embeddings['aquaculture'] == 1]\n",
    "    neg_samples = all_embeddings[all_embeddings['aquaculture'] == 0]\n",
    "\n",
    "    # Extract positive and negative samples from all_embeddings, put rest in test set\n",
    "    pos_train_indices = pos_samples.sample(n=min(positive_labels, len(pos_samples))).index\n",
    "    neg_train_indices = neg_samples.sample(n=min(negative_labels, len(neg_samples))).index\n",
    "    pos_test_indices = pos_samples.drop(pos_train_indices).index\n",
    "    neg_test_indices = neg_samples.drop(neg_train_indices).index\n",
    "\n",
    "    train_indices = np.concatenate((pos_train_indices, neg_train_indices))\n",
    "    test_indices = np.concatenate((pos_test_indices, neg_test_indices))\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    train_samples = all_embeddings.iloc[train_indices]\n",
    "    X_train = np.array(train_samples['embeddings'].tolist())\n",
    "    y_train = train_samples['aquaculture'].values\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    if drop_percentage > 0:\n",
    "        importances = rf.feature_importances_\n",
    "\n",
    "        # Calculate the number of features to keep based on the drop_percentage\n",
    "        num_features_to_keep = int(len(importances) * (1 - drop_percentage))\n",
    "        # Keep only the top important features based on the calculated number to keep\n",
    "        top_important_indices = np.argsort(importances)[::-1][:num_features_to_keep]\n",
    "        # Update all_embeddings to keep only the top important features\n",
    "        all_embeddings['embeddings'] = all_embeddings['embeddings'].apply(lambda x: x[top_important_indices])\n",
    "\n",
    "        train_samples = all_embeddings.loc[train_indices]\n",
    "        X_train = np.array(train_samples['embeddings'].tolist())\n",
    "        y_train = train_samples['aquaculture'].values\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "    # Get 25 closest cosine similarity to the centroid of the positive class, use pos_train_indices\n",
    "    centroid = np.mean(np.array(all_embeddings.iloc[pos_train_indices]['embeddings'].tolist()), axis=0)\n",
    "\n",
    "    # Calculate cosine similarity between the centroid and all embeddings\n",
    "    cosine_similarities = cosine_similarity([centroid], np.array(all_embeddings['embeddings'].tolist())).flatten()\n",
    "    \n",
    "    # Exclude the positive training set indices from the closest_indices\n",
    "    closest_indices = np.argsort(cosine_similarities)\n",
    "    closest_indices = [i for i in closest_indices if i not in pos_train_indices][:closest_similar]\n",
    "    \n",
    "    # Get the true labels and predicted labels for the closest indices\n",
    "    closest_true_labels = all_embeddings.iloc[closest_indices]['aquaculture'].values\n",
    "    closest_pred_labels = rf.predict(np.array(all_embeddings.iloc[closest_indices]['embeddings'].tolist()))\n",
    "    \n",
    "    # Calculate metrics for the 25 closest embeddings\n",
    "    closest_precision = precision_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_recall = recall_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_accuracy = accuracy_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_f1 = f1_score(closest_true_labels, closest_pred_labels)\n",
    "    \n",
    "    # Create a folium map to visualize the locations\n",
    "    m = folium.Map(location=[all_embeddings['y'].mean(), \n",
    "                             all_embeddings['x'].mean()], \n",
    "                   zoom_start=9, tiles=\"Esri.WorldImagery\")\n",
    "    \n",
    "    # Add markers for positive training locations\n",
    "    for idx in pos_train_indices:\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        geometry = row['geometry'] \n",
    "        folium.GeoJson(geometry, style_function=lambda x: {'color': 'green'}).add_to(m)\n",
    "    \n",
    "    # Add markers for negative training locations\n",
    "    for idx in neg_train_indices:\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        geometry = row['geometry']\n",
    "        folium.GeoJson(geometry, style_function=lambda x: {'color': 'red'}).add_to(m)\n",
    "    \n",
    "    # Add markers for closest predictions\n",
    "    for idx, true_label in zip(closest_indices, closest_true_labels):\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        geometry = row['geometry']\n",
    "        if true_label == 1:\n",
    "            # Correctly predicted positive\n",
    "            color = 'blue'\n",
    "            popup = f\"Closest True Positive: {idx}\"\n",
    "        else:\n",
    "            # Incorrectly predicted positive (false positive)\n",
    "            color = 'orange'\n",
    "            popup = f\"Closest False Positive: {idx}\"\n",
    "        folium.GeoJson(geometry, style_function=lambda x: {'color': color}).add_to(m)\n",
    "    \n",
    "    # Display the map\n",
    "    display(m)\n",
    "    \n",
    "    # Store the precision, number of dimensions, and iteration numbers\n",
    "    results = {\n",
    "        'precision': format(closest_precision, '.2f'),\n",
    "        'recall': format(closest_recall, '.2f'),\n",
    "        'accuracy': format(closest_accuracy, '.2f'),\n",
    "        'f1': format(closest_f1, '.2f'),\n",
    "        'num_positives': len(pos_train_indices),\n",
    "        'num_negatives': len(neg_train_indices),\n",
    "        'num_test': len(test_indices),\n",
    "        'num_dimensions': len(all_embeddings.iloc[0]['embeddings']),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "positive_labels = 10\n",
    "negative_labels = 20\n",
    "drop_percentage = 0.\n",
    "closest_similar = 25\n",
    "gdf = embeddings.copy().reset_index(drop=True)\n",
    "print(gdf.columns)\n",
    "\n",
    "results = calculate_precision(gdf, positive_labels, negative_labels, drop_percentage, closest_similar)\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def plot_satellite_image(bbox):\n",
    "    \"\"\"\n",
    "    Plot the satellite image of a given bounding box (bbox).\n",
    "    \n",
    "    Parameters:\n",
    "    - bbox (tuple): A tuple of (min_lat, min_lon, max_lat, max_lon)\n",
    "    \n",
    "    Returns:\n",
    "    - A folium map object with the satellite image of the given bbox.\n",
    "    \"\"\"\n",
    "    # Calculate the center of the bounding box\n",
    "    center_lat = (bbox[0] + bbox[2]) / 2\n",
    "    center_lon = (bbox[1] + bbox[3]) / 2\n",
    "    \n",
    "    # Create a folium map centered at the calculated center\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=13, tiles='Esri.WorldImagery', attr='Custom Attribution')\n",
    "    \n",
    "    # Add the bounding box as a rectangle on the map\n",
    "    folium.Rectangle(\n",
    "        bounds=[(bbox[0], bbox[1]), (bbox[2], bbox[3])],\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def find_most_common_closest(all_embeddings, pos_train_indices, closest_similar):\n",
    "    pos_train_embeddings = np.array(all_embeddings.iloc[pos_train_indices]['embeddings'].tolist())\n",
    "    \n",
    "    # Calculate cosine similarity between each positive training embedding and all embeddings\n",
    "    cosine_similarities = cosine_similarity(pos_train_embeddings, np.array(all_embeddings['embeddings'].tolist()))\n",
    "    \n",
    "    # Find the indices of the closest_similar samples for each positive training embedding\n",
    "    closest_indices_per_pos = np.argsort(cosine_similarities, axis=1)[:, -closest_similar:]\n",
    "    \n",
    "    # Flatten the closest indices and count their occurrences\n",
    "    closest_indices_flat = closest_indices_per_pos.flatten()\n",
    "    closest_indices_counts = np.bincount(closest_indices_flat)\n",
    "    \n",
    "    # Find the 25 most common closest indices\n",
    "    most_common_closest_indices = np.argsort(closest_indices_counts)[-25:]\n",
    "    \n",
    "    return most_common_closest_indices\n",
    "\n",
    "def calculate_precision(all_embeddings, positive_labels, negative_labels, \n",
    "                        drop_percentage, closest_similar):\n",
    "    all_embeddings.reset_index(drop=True, inplace=True)\n",
    "    pos_samples = all_embeddings[all_embeddings['aquaculture'] == 1]\n",
    "    neg_samples = all_embeddings[all_embeddings['aquaculture'] == 0] #take negative examples as negative closest in cosine similarity\n",
    "\n",
    "    # Extract positive and negative samples from all_embeddings, put rest in test set\n",
    "    pos_train_indices = pos_samples.sample(n=min(positive_labels, len(pos_samples))).index\n",
    "    neg_train_indices = neg_samples.sample(n=min(negative_labels, len(neg_samples))).index\n",
    "    pos_test_indices = pos_samples.drop(pos_train_indices).index\n",
    "    neg_test_indices = neg_samples.drop(neg_train_indices).index\n",
    "\n",
    "    train_indices = np.concatenate((pos_train_indices, neg_train_indices))\n",
    "    test_indices = np.concatenate((pos_test_indices, neg_test_indices))\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    train_samples = all_embeddings.iloc[train_indices]\n",
    "    X_train = np.array(train_samples['embeddings'].tolist())\n",
    "    y_train = train_samples['aquaculture'].values\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    if drop_percentage > 0:\n",
    "        importances = rf.feature_importances_\n",
    "\n",
    "        # Calculate the number of features to keep based on the drop_percentage\n",
    "        num_features_to_keep = int(len(importances) * (1 - drop_percentage))\n",
    "        # Keep only the top important features based on the calculated number to keep\n",
    "        top_important_indices = np.argsort(importances)[::-1][:num_features_to_keep]\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "   \n",
    "        # Sort importances\n",
    "        sorted_indices = np.argsort(importances)[::-1]\n",
    "        sorted_importances = importances[sorted_indices]\n",
    "\n",
    "        # Plot feature importances\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.bar(range(X_train.shape[1]), sorted_importances, align='center')\n",
    "\n",
    "        # Draw cutoff line\n",
    "        plt.axvline(x=num_features_to_keep-0.5, color='r', linestyle='--', label='Cutoff for feature selection')\n",
    "        plt.xticks(range(X_train.shape[1]), sorted_indices, rotation=90)\n",
    "        plt.xlim([-1, X_train.shape[1]])\n",
    "        plt.xlabel(\"Feature Index\")\n",
    "        plt.ylabel(\"Importance\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Update all_embeddings to keep only the top important features\n",
    "        all_embeddings['embeddings'] = all_embeddings['embeddings'].apply(lambda x: x[top_important_indices])\n",
    "\n",
    "        train_samples = all_embeddings.loc[train_indices]\n",
    "        X_train = np.array(train_samples['embeddings'].tolist())\n",
    "        y_train = train_samples['aquaculture'].values\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "    # Find the 25 most common closest samples to the positive training set\n",
    "    most_common_closest_indices = find_most_common_closest(all_embeddings, pos_train_indices, closest_similar)\n",
    "\n",
    "    # Get the true labels and predicted labels for the most common closest indices\n",
    "    closest_true_labels = all_embeddings.iloc[most_common_closest_indices]['aquaculture'].values\n",
    "    closest_pred_labels = rf.predict(np.array(all_embeddings.iloc[most_common_closest_indices]['embeddings'].tolist()))\n",
    "    \n",
    "    # Calculate metrics for the 25 most common closest embeddings\n",
    "    closest_precision = precision_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_recall = recall_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_accuracy = accuracy_score(closest_true_labels, closest_pred_labels)\n",
    "    closest_f1 = f1_score(closest_true_labels, closest_pred_labels)\n",
    "    \n",
    "    # Create a folium map to visualize the locations\n",
    "    m = folium.Map(location=[all_embeddings['y'].mean(), \n",
    "                             all_embeddings['x'].mean()], \n",
    "                   zoom_start=9, tiles=\"Esri.WorldImagery\")\n",
    "    \n",
    "    # Add markers for positive training locations\n",
    "    for idx in pos_train_indices:\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        geometry = row['geometry'] \n",
    "        folium.GeoJson(geometry, style_function=lambda x: {'color': 'green'}).add_to(m)\n",
    "    \n",
    "    # Add markers for negative training locations\n",
    "    for idx in neg_train_indices:\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        geometry = row['geometry']\n",
    "        folium.GeoJson(geometry, style_function=lambda x: {'color': 'red'}).add_to(m)\n",
    "    \n",
    "    # Add markers for most common closest predictions\n",
    "    for idx, true_label in zip(most_common_closest_indices, closest_true_labels):\n",
    "        row = all_embeddings.iloc[idx]\n",
    "        geometry = row['geometry']\n",
    "        if true_label == 1:\n",
    "            # Correctly predicted positive\n",
    "            color = 'blue'\n",
    "            popup = f\"Most Common Closest True Positive: {idx}\"\n",
    "        else:\n",
    "            # Incorrectly predicted positive (false positive)\n",
    "            color = 'orange'\n",
    "            popup = f\"Most Common Closest False Positive: {idx}\"\n",
    "        folium.GeoJson(geometry, style_function=lambda x: {'color': color}).add_to(m)\n",
    "    \n",
    "    # Display the map\n",
    "    display(m)\n",
    "    \n",
    "    # Store the precision, number of dimensions, and iteration numbers\n",
    "    results = {\n",
    "        'precision': format(closest_precision, '.2f'),\n",
    "        'recall': format(closest_recall, '.2f'),\n",
    "        'accuracy': format(closest_accuracy, '.2f'),\n",
    "        'f1': format(closest_f1, '.2f'),\n",
    "        'num_positives': len(pos_train_indices),\n",
    "        'num_negatives': len(neg_train_indices),\n",
    "        'num_test': len(test_indices),\n",
    "        'num_dimensions': len(all_embeddings.iloc[0]['embeddings']),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "positive_labels = 5\n",
    "negative_labels = 20\n",
    "drop_percentage = 0.5\n",
    "closest_similar = 10\n",
    "gdf = embeddings.copy().reset_index(drop=True)\n",
    "\n",
    "results = calculate_precision(gdf, positive_labels, negative_labels, drop_percentage, closest_similar)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "def find_most_common_closest(all_embeddings, train_indices, closest_similar):\n",
    "    train_embeddings = np.array(all_embeddings.iloc[train_indices]['embeddings'].tolist())\n",
    "    # Calculate cosine similarity between each positive training embedding and all embeddings\n",
    "    cosine_similarities = cosine_similarity(train_embeddings, np.array(all_embeddings['embeddings'].tolist()))\n",
    "    # Find the indices of the closest_similar samples for each positive training embedding\n",
    "    closest_indices_per_pos = np.argsort(cosine_similarities, axis=1)[:, -closest_similar:]\n",
    "\n",
    "    for train_idx, closest_idxs in zip(train_indices, closest_indices_per_pos):\n",
    "        train_aquaculture = all_embeddings.iloc[train_idx]['aquaculture']\n",
    "        closest_aquacultures = all_embeddings.iloc[closest_idxs]['aquaculture'].values\n",
    "        same_aquaculture_count = np.sum(closest_aquacultures == train_aquaculture)\n",
    "        print(same_aquaculture_count)\n",
    "\n",
    "    # Flatten the closest indices and count their occurrences\n",
    "    closest_indices_flat = closest_indices_per_pos.flatten()\n",
    "    closest_indices_counts = np.bincount(closest_indices_flat)\n",
    "    # Find the 25 most common closest indices\n",
    "    most_common_closest_indices = np.argsort(closest_indices_counts)[-25:]\n",
    "    \n",
    "    return most_common_closest_indices\n",
    "\n",
    "def calculate_precision(all_embeddings,\n",
    "                        positive_labels, negative_labels, drop_percentage,\n",
    "                        do_map=False,\n",
    "                        plot_features=True):\n",
    "    \n",
    "    all_embeddings.reset_index(drop=True, inplace=True)\n",
    "    pos_samples = all_embeddings[all_embeddings['aquaculture'] == 1]\n",
    "    neg_samples = all_embeddings[all_embeddings['aquaculture'] == 0]\n",
    "\n",
    "    pos_train_indices = pos_samples.sample(n=min(positive_labels, len(pos_samples))).index\n",
    "    neg_train_indices = neg_samples.sample(n=min(negative_labels, len(neg_samples))).index\n",
    "    pos_test_indices = pos_samples.drop(pos_train_indices).index\n",
    "    neg_test_indices = neg_samples.drop(neg_train_indices).index\n",
    "\n",
    "    train_indices = np.concatenate((pos_train_indices, neg_train_indices))\n",
    "    test_indices = np.concatenate((pos_test_indices, neg_test_indices))\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    train_samples = all_embeddings.iloc[train_indices]\n",
    "    X_train = np.array(train_samples['embeddings'].tolist())\n",
    "    y_train = train_samples['aquaculture'].values\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    if drop_percentage > 0:\n",
    "        importances = rf.feature_importances_\n",
    "        num_features_to_keep = int(len(importances) * (1 - drop_percentage))\n",
    "        sorted_indices = np.argsort(importances)[::-1]\n",
    "        sorted_importances = importances[sorted_indices]\n",
    "        top_important_indices = sorted_importances[:num_features_to_keep]\n",
    "        top_important_indices = top_important_indices.astype(int)\n",
    "\n",
    "        if plot_features:\n",
    "            # Plot feature importances\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.title(\"Feature Importances\")\n",
    "            plt.bar(range(len(sorted_importances)), sorted_importances, align='center')\n",
    "            plt.yscale('log')\n",
    "\n",
    "            # Set x-tick labels to the sorted feature indices\n",
    "            plt.xticks(range(len(importances)), sorted_indices, rotation=90)\n",
    "            plt.axvline(x=num_features_to_keep-0.5, color='r', linestyle='--', label='Cutoff for feature selection')\n",
    "\n",
    "            plt.xlabel(\"Feature Index [Log]\")\n",
    "            plt.ylabel(\"Importance\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        all_embeddings['embeddings'] = all_embeddings['embeddings'].apply(lambda x: x[top_important_indices])\n",
    "        \n",
    "        #pull again the train samples and RF from the pruned embeddings\n",
    "        train_samples = all_embeddings.loc[train_indices]\n",
    "        X_train = np.array(train_samples['embeddings'].tolist())\n",
    "        y_train = train_samples['aquaculture'].values\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "    # Find the 25 most common closest samples to the positive training set\n",
    "    most_common_closest_indices = find_most_common_closest(all_embeddings, pos_train_indices, closest_similar)\n",
    "    # Find the 25 most common closest samples to the negative training set\n",
    "    most_common_closest_indices_neg = find_most_common_closest(all_embeddings, neg_train_indices, closest_similar)\n",
    "    #remove most common closest negative indices from the most common closest indices, if they exist\n",
    "    most_common_closest_indices = [i for i in most_common_closest_indices if i not in most_common_closest_indices_neg]\n",
    "\n",
    "    #count how many of the most common closest indices are actually positive\n",
    "    print(\"Number of positive samples in the most common closest indices\")\n",
    "    print(len(all_embeddings[all_embeddings.index.isin(most_common_closest_indices) & all_embeddings['aquaculture'] == 1]))\n",
    "\n",
    "    # Get the true labels and predicted labels for the most common closest indices\n",
    "    closest_true_labels = all_embeddings.iloc[most_common_closest_indices]['aquaculture'].values\n",
    "    closest_pred_labels = np.ones(len(most_common_closest_indices))\n",
    "\n",
    "    # Calculate metrics for the 25 most common closest embeddings\n",
    "    precision = precision_score(closest_true_labels, closest_pred_labels)\n",
    "    recall = recall_score(closest_true_labels, closest_pred_labels)\n",
    "    accuracy = accuracy_score(closest_true_labels, closest_pred_labels)\n",
    "    f1 = f1_score(closest_true_labels, closest_pred_labels)\n",
    "\n",
    "    if do_map:\n",
    "        m = folium.Map(location=[all_embeddings['y'].mean(), \n",
    "                                    all_embeddings['x'].mean()], \n",
    "                        zoom_start=9, tiles=\"Esri.WorldImagery\")\n",
    "\n",
    "        for idx in pos_train_indices:\n",
    "            row = all_embeddings.iloc[idx]\n",
    "            geometry = row['geometry'] \n",
    "            folium.GeoJson(geometry, style_function=lambda x: {'color': 'blue'}).add_to(m)\n",
    "\n",
    "        for idx in neg_train_indices:\n",
    "            row = all_embeddings.iloc[idx]\n",
    "            geometry = row['geometry']\n",
    "            folium.GeoJson(geometry, style_function=lambda x: {'color': 'red'}).add_to(m)\n",
    "\n",
    "        # Add markers for most common closest predictions\n",
    "        for idx, true_label in zip(most_common_closest_indices, closest_true_labels):\n",
    "            print(idx, true_label)\n",
    "            row = all_embeddings.iloc[idx]\n",
    "            geometry = row['geometry']\n",
    "            if true_label == 1:\n",
    "                color = 'yellow' #true positive\n",
    "            elif true_label == 0:\n",
    "                color = 'orange' #false positive\n",
    "            else:\n",
    "                break\n",
    "            folium.GeoJson(geometry, style_function=lambda x: {'color': color}).add_to(m)\n",
    "\n",
    "        display(m)\n",
    "    \n",
    "    results = {\n",
    "        'precision': format(precision, '.2f'),\n",
    "        'recall': format(recall, '.2f'),\n",
    "        'accuracy': format(accuracy, '.2f'),\n",
    "        'f1': format(f1, '.2f'),\n",
    "        'num_positives': len(pos_train_indices),\n",
    "        'num_negatives': len(neg_train_indices),\n",
    "        'num_test': len(test_indices),\n",
    "        'num_dimensions': len(all_embeddings.iloc[0]['embeddings']),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "positive_labels = 5\n",
    "negative_labels = 3\n",
    "drop_percentage = 0\n",
    "closest_similar = 10\n",
    "gdf = embeddings.copy().reset_index(drop=True)\n",
    "calculate_precision(gdf, positive_labels, negative_labels, drop_percentage, closest_similar, do_map = True, plot_features = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8baf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(embeddings, drop_percentages, positive_counts):\n",
    "    results = []\n",
    "    for drop_percentage in drop_percentages:\n",
    "        for positive_count in positive_counts:\n",
    "            negative_count = 2* positive_count\n",
    "            result = calculate_precision(embeddings.copy(), positive_count, negative_count, drop_percentage)\n",
    "            result['drop_percentage'] = drop_percentage\n",
    "            result['positive_count'] = positive_count\n",
    "            results.append(result)\n",
    "            print(f\"Drop Percentage: {drop_percentage}, Positive Count: {positive_count}\")\n",
    "            print(result)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "drop_percentages = [i/100 for i in range(0, 71, 10)]\n",
    "positive_counts = list(range(1, 101, 10))\n",
    "\n",
    "results_df = run_experiment(embeddings, drop_percentages, positive_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for drop_percentage in drop_percentages:\n",
    "    plt.scatter(results_df[results_df['drop_percentage'] == drop_percentage]['positive_count'],\n",
    "                results_df[results_df['drop_percentage'] == drop_percentage]['precision'],\n",
    "                label=f\"Drop Percentage: {drop_percentage}\")\n",
    "plt.xlabel(\"Number of Positive Cases\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(title=\"Drop Percentage\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
